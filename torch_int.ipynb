{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bce50f-66e5-4687-bc50-864209cd3d92",
   "metadata": {},
   "source": [
    "https://github.com/tcwangshiqi-columbia/symbolic_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7988edf2-d3e1-41f1-bb1d-281a41738466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56595089-bc14-4609-bb92-8d6b9eb7b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interval():\n",
    "    '''Naive interval class\n",
    "    Naive interval propagation is low-cost (only around two times slower \n",
    "    than regular NN propagation). However, the output range provided is \n",
    "    loose. This is because the dependency of inputs are ignored.\n",
    "    See ReluVal https://arxiv.org/abs/1804.10829 for more details of\n",
    "    the tradeoff.\n",
    "    Naive interval propagation are used for many existing training\n",
    "    schemes:\n",
    "    (1) DiffAi: http://proceedings.mlr.press/v80/mirman18b/mirman18b.pdf\n",
    "    (2) IBP: https://arxiv.org/pdf/1810.12715.pdf\n",
    "    These training schemes are fast but the robustness of trained models\n",
    "    suffers from the loose estimations of naive interval propagation.\n",
    "\n",
    "    Args:\n",
    "        lower: numpy matrix of the lower bound for each layer nodes\n",
    "        upper: numpy matrix of the upper bound for each layer nodes\n",
    "        lower and upper should have the same shape of input for \n",
    "        each layer\n",
    "        no upper value should be less than corresponding lower value\n",
    "    * :attr:`l` and `u` keeps the upper and lower values of the\n",
    "      interval. Naive interval propagation using them to propagate.\n",
    "    * :attr:`c` and `e` means the center point and the error range \n",
    "      of the interval. Symbolic interval propagation using to propagate\n",
    "      since it can keep the dependency more efficiently. \n",
    "    * :attr:`mask` is used to keep the estimation information for each\n",
    "      hidden node. It has the same shape of the ReLU layer input. \n",
    "      for each hidden node, before going through ReLU, let [l,u] denote\n",
    "      a ReLU's input range. It saves the value u/(u-l), which is the\n",
    "      slope of estimated output dependency. 0 means, given the input\n",
    "      range, this ReLU's input will always be negative and the output \n",
    "      is always 0. 1 indicates, it always stays positive and the\n",
    "      output will not change. Otherwise, this node is estimated during \n",
    "      interval propagation and will introduce overestimation error. \n",
    "    '''\n",
    "    def __init__(self, lower, upper, use_cuda=False):\n",
    "        if(not isinstance(self, Inverse_interval)):\n",
    "            assert not ((upper-lower)<0).any(), \"upper less than lower\"\n",
    "        self.l = lower\n",
    "        self.u = upper\n",
    "        self.c = (lower+upper)/2\n",
    "        self.e = (upper-lower)/2\n",
    "        self.mask = []\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "\n",
    "    def update_lu(self, lower, upper):\n",
    "        '''Update this interval with new lower and upper numpy matrix\n",
    "        Args:\n",
    "            lower: numpy matrix of the lower bound for each layer nodes\n",
    "            upper: numpy matrix of the upper bound for each layer nodes\n",
    "        '''\n",
    "        if(not isinstance(self, Inverse_interval)):\n",
    "            assert not ((upper-lower)<0).any(), \"upper less than lower\"\n",
    "        self.l = lower\n",
    "        self.u = upper\n",
    "        self.c = (lower+upper)/2\n",
    "        self.e = (upper-lower)/2\n",
    "\n",
    "\n",
    "    def update_ce(self, center, error):\n",
    "        '''Update this interval with new error and center numpy matrix\n",
    "        Args:\n",
    "            lower: numpy matrix of the lower bound for each layer nodes\n",
    "            upper: numpy matrix of the upper bound for each layer nodes\n",
    "        '''\n",
    "        if(not isinstance(self, Inverse_interval)):\n",
    "            assert not (error<0).any(), \"upper less than lower\"\n",
    "        self.c = center\n",
    "        self.e = error\n",
    "        self.u = self.c+self.e\n",
    "        self.l = self.c-self.e\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        '''Print function\n",
    "        '''\n",
    "        string = \"interval shape:\"+str(self.c.shape)\n",
    "        string += \"\\nlower:\"+str(self.l)\n",
    "        string += \"\\nupper:\"+str(self.u)\n",
    "        return string\n",
    "\n",
    "    def worst_case(self, y, output_size):\n",
    "        '''Calculate the wrost case of the analyzed output ranges.\n",
    "        In details, it returns the upper bound of other label minus \n",
    "        the lower bound of the target label. If the returned value is \n",
    "        less than 0, it means the worst case provided by interval\n",
    "        analysis will never be larger than the target label y's. \n",
    "        '''\n",
    "        assert y.shape[0] == self.l.shape[0] == self.u.shape[0],\\\n",
    "                \"wrong input shape\"\n",
    "        \n",
    "        for i in range(y.shape[0]):\n",
    "            t = self.l[i, y[i]]\n",
    "            self.u[i] = self.u[i]-t\n",
    "            self.u[i, y[i]] = 0.0\n",
    "        return self.u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eee6dae-5194-4783-aff7-60f27c55e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interval_network(nn.Module):\n",
    "    '''Convert a nn.Sequential model to a network support symbolic\n",
    "    interval propagations/naive interval propagations.\n",
    "    '''\n",
    "    def __init__(self, model, c):\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.net = []\n",
    "        first_layer = True\n",
    "        last_layer = False\n",
    "\n",
    "        for layer in model:\n",
    "            if(isinstance(layer, nn.Linear)):\n",
    "                if layer == model[-1]:\n",
    "                    last_layer = True\n",
    "                if last_layer and c is not None:\n",
    "                    wc_matrix = c\n",
    "                else:\n",
    "                    wc_matrix = None\n",
    "                self.net.append(Interval_Dense(layer, first_layer, wc_matrix=wc_matrix))\n",
    "                first_layer = False\n",
    "            if(isinstance(layer, nn.ReLU)):\n",
    "                self.net.append(Interval_ReLU(layer))\n",
    "            if(isinstance(layer, nn.Conv2d)):\n",
    "                self.net.append(Interval_Conv2d(layer, first_layer))\n",
    "                first_layer = False\n",
    "            if 'Flatten' in (str(layer.__class__.__name__)): \n",
    "                self.net.append(Interval_Flatten())\n",
    "            if 'Vlayer' in (str(layer.__class__.__name__)): \n",
    "                self.net.append(Interval_Vlayer(layer))\n",
    "            if 'bn' in (str(layer.__class__.__name__)):\n",
    "                self.net.append(Interval_BN(layer))\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "    '''Forward intervals for each layer.\n",
    "    * :attr:`ix` is the input fore each layer. If ix is a naive\n",
    "    interval, it will propagate naively. If ix is a symbolic \n",
    "    interval, it will propagate symbolicly.\n",
    "    '''\n",
    "    def forward(self, ix):\n",
    "        return self.net(ix)\n",
    "        '''\n",
    "        for i, layer in enumerate(self.net):\n",
    "            ix = layer(ix)\n",
    "        return ix\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2bdfd58-6c68-4f74-afd5-c97be6628b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interval_Dense(nn.Module):\n",
    "    def __init__(self, layer, first_layer=False, wc_matrix=None):\n",
    "        nn.Module.__init__(self)\n",
    "        self.layer = layer\n",
    "        self.first_layer = first_layer\n",
    "        self.wc_matrix = wc_matrix\n",
    "\n",
    "    def forward(self, ix):\n",
    "        assert isinstance(ix, Interval), \"Not Interval instance\"\n",
    "\n",
    "        c = ix.c\n",
    "        e = ix.e\n",
    "        if self.wc_matrix is None:\n",
    "            c = F.linear(c, self.layer.weight, bias=self.layer.bias)\n",
    "            e = F.linear(e, self.layer.weight.abs())\n",
    "        else:\n",
    "            weight = self.wc_matrix.matmul(self.layer.weight)\n",
    "            bias = self.wc_matrix.matmul(self.layer.bias)\n",
    "            \n",
    "            c = weight.matmul(c.unsqueeze(-1)) + bias.unsqueeze(-1)\n",
    "            e = weight.abs().matmul(e.unsqueeze(-1))\n",
    "\n",
    "            c, e = c.squeeze(-1), e.squeeze(-1)\n",
    "\n",
    "        #print(c.shape, e.shape)\n",
    "        #print(\"naive e\", e)\n",
    "        #print(\"naive c\", c)\n",
    "        ix.update_lu(c-e, c+e)\n",
    "        return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bc336bd-3cb0-4e28-81da-6f35ce4214b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interval_Conv2d(nn.Module):\n",
    "    def __init__(self, layer, first_layer=False):\n",
    "        nn.Module.__init__(self)\n",
    "        self.layer = layer\n",
    "        self.first_layer = first_layer\n",
    "        #print (\"conv2d:\", self.layer.weight.shape)\n",
    "\n",
    "    def forward(self, ix):\n",
    "        assert isinstance(ix, Interval), \"Not Interval instance\"\n",
    "\n",
    "        c = ix.c\n",
    "        e = ix.e\n",
    "        c = F.conv2d(c, self.layer.weight, \n",
    "                       stride=self.layer.stride,\n",
    "                       padding=self.layer.padding, \n",
    "                       bias=self.layer.bias)\n",
    "        e = F.conv2d(e, self.layer.weight.abs(), \n",
    "                       stride=self.layer.stride,\n",
    "                       padding=self.layer.padding)\n",
    "        ix.update_lu(c-e, c+e)\n",
    "        return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9219dde-ece5-4960-b197-ebd307791f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interval_BN(nn.Module):\n",
    "    def __init__(self, layer, first_layer=False):\n",
    "        nn.Module.__init__(self)\n",
    "        self.layer = layer\n",
    "        self.first_layer = first_layer\n",
    "\n",
    "    def forward(self, ix):\n",
    "        assert isinstance(ix, Interval), \"Not Interval instance\"\n",
    "\n",
    "        shape = ix.u.shape\n",
    "        tmax = torch.where(ix.u>-ix.l, ix.u, ix.l).view(ix.batch_size, -1)\n",
    "        #tmax = ix.c.view(ix.batch_size, -1)\n",
    "        mean = tmax.mean(dim=0, keepdim=True)\n",
    "        # print(mean.shape, tmax.shape, ix.u.shape)\n",
    "        \n",
    "        sigma = torch.norm(tmax-mean, dim=0, keepdim=True)\n",
    "        #sigma = sigma*sigma\n",
    "\n",
    "        # if self.layer.mean is None:\n",
    "        #     self.layer.mean = mean\n",
    "        #     self.layer.sigma = sigma\n",
    "        # else:\n",
    "        \n",
    "        # self.layer.mean = self.layer.mean * (1-self.layer.momentum) + self.layer.momentum * mean\n",
    "        # self.layer.sigma = self.layer.sigma * (1-self.layer.momentum) + self.layer.momentum * sigma\n",
    "\n",
    "        self.layer.mean = mean\n",
    "        self.layer.sigma = sigma\n",
    "        #print(mean, sigma)\n",
    "\n",
    "        ix.u = (ix.u.view(ix.batch_size, -1)-mean)/sigma\n",
    "        ix.l = (ix.l.view(ix.batch_size, -1)-mean)/sigma\n",
    "        ix.u, ix.l = ix.u.view(shape), ix.l.view(shape)\n",
    "        return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85c81ead-b9a8-4ec9-91bd-f8e21ee0cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interval_ReLU(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        nn.Module.__init__(self)\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self, ix):\n",
    "        #print(ix.u)\n",
    "        #print(ix.l)\n",
    "        assert isinstance(ix, Interval), \"Not Interval instance\"\n",
    "\n",
    "        '''\n",
    "        lower = ix.l.clamp(max=0)\n",
    "        upper = ix.u.clamp(min=0)\n",
    "        upper = torch.max(upper, lower + 1e-8)\n",
    "        mask = upper / (upper - lower)\n",
    "        ix.mask.append(mask)\n",
    "        '''\n",
    "        ix.update_lu(F.relu(ix.l), F.relu(ix.u))\n",
    "        return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02679ee9-73e2-46fc-ab0d-5d64f9340825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interval_Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "    def forward(self, ix):\n",
    "        assert isinstance(ix, Interval), \"Not Interval instance\"\n",
    "        ix.update_lu(ix.l.view(ix.l.size(0), -1),\\\n",
    "            ix.u.view(ix.u.size(0), -1))\n",
    "        return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f81cb5db-fded-4073-b57f-bc4c34a54b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interval_Bound(nn.Module):\n",
    "    def __init__(self, net, epsilon, method=\"sym\",\\\n",
    "                    proj=None, use_cuda=True, norm=\"linf\", worst_case=True):\n",
    "        nn.Module.__init__(self)\n",
    "        self.net = net\n",
    "        self.epsilon = epsilon\n",
    "        self.use_cuda = use_cuda\n",
    "        self.proj = proj\n",
    "        if(proj is not None):\n",
    "            assert proj>0, \"project dimension has to be larger than 0,\"\\\n",
    "                    \" please use naive bound propagation (proj=0)!\"\n",
    "            assert (isinstance(proj, int)), \"project dimension has to\"\\\n",
    "                    \" be integer!\"\n",
    "\n",
    "        assert method in [\"sym\", \"naive\", \"inverse\", \"center_sym\", \"new\", \"gen\", \"mix\"],\\\n",
    "                \"No such interval methods!\"\n",
    "        self.method = method\n",
    "        self.norm = norm\n",
    "        #assert self.norm in [\"linf\", \"l2\", \"l1\"], \"norm\" + norm + \"not supported\"\n",
    "\n",
    "        self.worst_case = worst_case\n",
    "            \n",
    "    def forward(self, X, y):\n",
    "        \n",
    "        out_features = self.net[-1].out_features\n",
    "\n",
    "        if self.worst_case:\n",
    "            c = torch.eye(out_features).type_as(X)[y].unsqueeze(1) -\\\n",
    "                    torch.eye(out_features).type_as(X).unsqueeze(0)\n",
    "        else:\n",
    "            c = None\n",
    "\n",
    "        # Transfer original model to interval models\n",
    "        inet = Interval_network(self.net, c)\n",
    "        \n",
    "        minimum = X.min().item()\n",
    "        maximum = X.max().item()\n",
    "\n",
    "        # Create symbolic inteval classes from X\n",
    "        assert self.method == \"naive\", \"Not naive method\"\n",
    "        ix = Interval(torch.clamp(X-self.epsilon, minimum, maximum),\\\n",
    "                    torch.clamp(X+self.epsilon, minimum, maximum),\\\n",
    "                    elf.use_cuda\\\n",
    "                )\n",
    "        # Propagate symbolic interval through interval networks\n",
    "        ix = inet(ix)\n",
    "        # print(ix.u)\n",
    "        # print(ix.l)\n",
    "        return -ix.l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a5364bd-5a5b-415a-bb89-f040cbae75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_interval_analyze(net, epsilon, X, y,\\\n",
    "                    use_cuda=True, parallel=False, norm=\"linf\"):\n",
    "\n",
    "    # Transfer original model to interval models\n",
    "\n",
    "    if(parallel):\n",
    "        wc = nn.DataParallel(Interval_Bound(net, epsilon,\n",
    "                method=\"naive\", use_cuda=use_cuda, norm=norm))(X, y)\n",
    "    else:\n",
    "        wc = Interval_Bound(net, epsilon, method=\"naive\",\\\n",
    "                        use_cuda=use_cuda, norm=norm)(X, y)\n",
    "\n",
    "    iloss = nn.CrossEntropyLoss()(wc, y)\n",
    "    ierr = (wc.max(1)[1]!=y).type(torch.Tensor)\n",
    "    ierr = ierr.sum().item()/X.shape[0]\n",
    "\n",
    "    return iloss, ierr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
